% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/edgenet_modelselection.R
\name{cv.edgenet}
\alias{cv.edgenet}
\title{Find the optimal shrinkage parameters for edgenet}
\usage{
cv.edgenet(X, Y, G.X = NULL, G.Y = NULL, lambda = NA_real_,
  psigx = NA_real_, psigy = NA_real_, thresh = 1e-05,
  maxit = 1e+05, learning.rate = 0.01, family = gaussian,
  optim.maxit = 100, nfolds = 10)
}
\arguments{
\item{X}{input matrix, of dimension (\code{n} x \code{p})
where \code{n} is the number of observations and \code{p} is the number
of covariables. Each row is an observation vector.}

\item{Y}{output matrix, of dimension (\code{n} x \code{q})
where \code{n} is the number of observations and \code{q} is the number
of response variables Each row is an observation vector.}

\item{G.X}{non-negativ affinity matrix for \code{n}, of dimensions
(\code{p} x \code{p}) where \code{p} is the number of covariables \code{X}.
Providing a graph \code{G.X} will optimize the regularization
parameter \code{psi.gx}. If this is not desired just set \code{G.X} to
\code{NULL}.}

\item{G.Y}{non-negativ affinity matrix for \code{n}, of dimensions
(\code{q} x \code{q}) where \code{q} is the number of responses \code{Y}.
Providing a graph \code{G.Y} will optimize the regularization
parameter \code{psi.gy}. If this is not desired just set \code{G.Y} to
\code{NULL}.}

\item{lambda}{\code{numerical} shrinkage parameter for LASSO. Per default
this parameter is
 set to \code{NULL} which means that \code{lambda} is going to be estimated
 using cross-validation. If any \code{numerical} value for \code{lambda}
 is set, estimation of the optimal parameter will \emph{not} be conducted.}

\item{psigx}{\code{numerical} shrinkage parameter for graph-regularization
of \code{G.X}. Per default this parameter is
set to \code{NULL} which means that \code{psigx} is going to be estimated
in the cross-validation. If any \code{numerical} value for \code{psigx} is
set, estimation of the optimal parameter will \emph{not} be conducted.}

\item{psigy}{\code{numerical} shrinkage parameter for graph-regularization
of \code{G.Y}. Per default this parameter is
set to \code{NULL} which means that \code{psigy} is going to be estimated
in the cross-validation. If any \code{numerical} value for \code{psigy} is
set, estimation of the optimal parameter will \emph{not} be conducted.}

\item{thresh}{\code{numerical} threshold for coordinate descent}

\item{maxit}{maximum number of iterations for the coordinate descent
(\code{integer})}

\item{family}{family of response, e.g. \emph{gaussian}}

\item{optim.maxit}{the maximum number of iterations for the optimization
(\code{integer}). Usually 1e4 is a good choice.}

\item{nfolds}{the number of folds to be used - default is 10
(minimum 3, maximum \code{nrow(X)}).}

\item{optim.epsilon}{\code{numerical} threshold criterion for the
optimization to stop.  Usually 1e-3 is a good choice.}
}
\value{
An object of class \code{cv.edgenet}
\item{call }{ the call that produced the object}
\item{lambda }{ the estimated (\code{p} x \code{q})-dimensional
 coefficient matrix B.hat}
\item{psigx }{ the estimated (\code{q} x \code{1})-dimensional
 vector of intercepts}
\item{psigy }{ the estimated (\code{q} x \code{1})-dimensional vector
 of intercepts}
}
\description{
Finds the optimal shrinkage parameters
 using cross-validation for edgenet. We use the BOBYQA algorithm to
 find the optimial regularization parameters and coordinate
 descent in order to minimize the objective function of the linear model.
}
\examples{
X <- matrix(rnorm(100*10), 100, 10)
b <- rnorm(10)
G.X <- matrix(rpois(10*10,1),10)
G.X <- t(G.X) + G.X
diag(G.X) <- 0

# fit a Gaussian model
Y <- X\%*\%b + rnorm(100)
cv.edge <- cv.edgenet(X=X, Y=Y, G.X=G.X, family="gaussian")
}
\references{
Dirmeier, Simon and Fuchs, Christiane and Mueller, Nikola S and Theis,
 Fabian J (2018),
 netReg: Network-regularized linear models for biological association
 studies. \cr
 Friedman J., Hastie T., Hoefling H. and Tibshirani R. (2007),
 Pathwise coordinate optimization.\cr
 \emph{The Annals of Applied Statistics}\cr \cr
 Friedman J., Hastie T. and Tibshirani R. (2010),
 Regularization Paths for Generalized Linear Models via
  Coordinate Descent. \cr
 \emph{Journal of Statistical Software}\cr \cr
 Fu W. J. (1998),  Penalized Regression: The Bridge Versus the Lasso.\cr
 \emph{Journal of Computational and Graphical Statistics}\cr \cr
 Cheng W. and Wang W. (2014), Graph-regularized dual Lasso for
  robust eQTL mapping.\cr
 \emph{Bioinformatics}\cr \cr
 Powell M.J.D. (2009),
 The BOBYQA algorithm for bound constrained optimization without
  derivatives.\cr
 \url{http://www.damtp.cam.ac.uk/user/na/NA_papers/NA2009_06.pdf}
}
