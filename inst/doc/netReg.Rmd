---
title: "netReg"
author: "Simon Dirmeier"
date: "`r Sys.Date()`"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{netReg}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Introduction

**netReg** is a package for graph-regularized linear regression. Graph prior knowledge is incorporated into the likelihood of the linear model in the form of the Laplacian matrices of the graphs. The Laplacians penalize the estimation of the coefficients for smooth solutions. 

For high-dimensional data-sets often no unique solutions exist. With graph-regularization high-dimensional data-sets can be fitted efficiently.


## Edgenet tutorial

This section explains how to fit a linear model and do parameter estimation using `edgenet`-regularization.

At first we generate some toy data randomly:

```{r}
  X <- matrix(rnorm(100*5), 100)
  Y <- matrix(rnorm(100*5), 100)
```

Then we load the `netReg` library

```{r}
  library(netReg)
```

For `edgenet` we need to create an affinity matrix for the co-variables first. We also can create a graph for the responses, but this is not necessary to demonstrate the method. We could create a random graph like this:

```{r}
  aff.mat <- matrix(rpois(5*5, 1),5) * abs(rnorm(5*5))
  aff.mat <- t(aff.mat) + aff.mat
  diag(aff.mat) <- 0
```

We created the affinity matrix absolutely random, although in practice a *real* (biological) observed affinity matrix should be used, because in the end the affinity matrix decides the shrinkage of the coefficients. 

### Model fitting

Fitting a model using edge-based regularization is done with:

```{r}
  fit <- edgenet(X=X, Y=Y, G.X=aff.mat, lambda=1, psigx=1, family="gaussian")
  print(fit)
```

The `fit` object contains information about coefficients, intercepts, residuals, etc. Having the coefficients estimated we are able to predict novel data-sets:

```{r}
  X.new <-  matrix(rnorm(10*5),10)
  pred  <- predict(fit, X.new)
```

The `pred` objects contains the predicted values for the responses.

### Model-selection

In some cases we do not know the optimal shrinkage parameters. In that case we can just use `cv.edgenet` to find the best parameters for us. With *best* parameters we mean the ones reducing the *residual sum of squares* for Gaussian regression models:

```{r}
  cv <- cv.edgenet(X=X, Y=Y, G.X=aff.mat, family="gaussian")
  print(cv)
```

## References

<p>Friedman J., Hastie T., Hoefling H. and Tibshirani R. (2007), Pathwise coordinate optimization.<br>
  <i>The Annals of Applied Statistics</i>
</p>  
<p>Friedman J., Hastie T. and Tibshirani R. (2010), Regularization Paths for Generalized Linear Models via Coordinate Descent.<br>
  <i>Journal of statistical software</i>
</p>
<p>Fu W. J. (1998), Penalized Regression: The Bridge Versus the Lasso.<br>
  <i>Journal of computational and graphical statistics</i>
</p>
<p>Cheng W. and Wang W. (2014), Graph-regularized dual Lasso for robust eQTL mapping.<br>
  <i>Bioinformatics</i>
</p>