g.x <- matrix(runif(p * p), p)
g.x <- t(g.x) + g.x
g.x
g.x == t(g.x)
x <- matrix(rnorm(n * p), n)
y <- x %*% rnorm(p) + rnorm(n)
y
x <- cast_float(x)
source('~/PROJECTS/netReg/R/utils_tf.R')
x <- cast_float(x)
y <- cast_float(y)
x <- cast_float(x)
y <- cast_float(y)
if (!is.null(gx)) {
gx <- cast_float(laplacian_(gx))
}
gx <- t(g.x) + g.x
if (!is.null(gx)) {
gx <- cast_float(laplacian_(gx))
}
source('~/PROJECTS/netReg/R/RcppExports.R')
if (!is.null(gx)) {
gx <- cast_float(laplacian_(gx))
}
install.packages("nloptr")
install.packages(c("Rcpp", "RcppArmadillo", "grplasso", "LaplacesDemon", "styler", "lintr", "knitr"))
install.packages(c("Rcpp", "RcppArmadillo", "grplasso", "LaplacesDemon", "styler", "lintr", "knitr"))
library(netReg)
library(tfprobability)
reticulate::py_module_available("tensorflow_probability
reticulate::py_module_available("tensorflow_probability")
reticulate::py_module_available("tensorflow-probability")
reticulate::py_module_available("tensorflowprobability")
reticulate::py_module_available("tensorflow")
reticulate::py_module_available
?reticulate::py_module_available(module = modul)
reticulate::conda_list()
reticulate::conda_list(conda = "r-reticulate")
py_module_available
?py_module_available
library(reticulate)
py_module_available()
?py_module_available()
py_module_available("tensorflow_probability")
py_module_available("tensorflow-probability")
gx <- cast_float(laplacian_(gx))
# TODO: think about this
alpha <- zero_vector(q) + 1
beta <- zero_matrix(p, q) + 1
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
source('~/PROJECTS/netReg/R/loss.R')
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
source('~/PROJECTS/netReg/R/util.R')
family <- get.family("gaussian")
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
gy
gy <- NULL
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
loss
alpha
lambda
lambda <- psigx <- 1
psigy <- 0
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
source('~/PROJECTS/netReg/R/predictor.R')
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
gy <- NULL
n <- 100
p <- 10
psigx <- lambda <- 1
psigy <- 0
g.x <- matrix(runif(p * p), p)
gx <- t(g.x) + g.x
gy <- NULL
x <- matrix(rnorm(n * p), n)
y <- x %*% rnorm(p) + rnorm(n)
library(tensorflow)
library(tfprobability)
source('~/PROJECTS/netReg/R/utils_tf.R')
source('~/PROJECTS/netReg/R/predictor.R')
source('~/PROJECTS/netReg/R/loss.R')
source('~/PROJECTS/netReg/R/util.R')
p <- ncol(x)
q <- ncol(y)
x <- cast_float(x)
y <- cast_float(y)
if (!is.null(gx)) {
gx <- cast_float(laplacian_(gx))
}
if (!is.null(gy)) {
gy <- cast_float(laplacian_(gy))
}
gy
gx <- NULL
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
family
family <- get.family("gaussian")
family
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
loss
alpha
alpha <- zero_vector(q) + 1
beta <- zero_matrix(p, q) + 1
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
source('~/PROJECTS/netReg/R/loss.R')
source('~/PROJECTS/netReg/R/likelihood.R')
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
loss
family$loss
get.family("gaussian")
s <- get.family("gaussian")
get.family
s
class(s)
source('~/PROJECTS/netReg/R/family.R')
get.family("gaussian")
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
family <- get.family("gaussian")
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
loss
.lasso.penalty()
.lasso.penalty(1)
.lasso.penalty(1, 2, 2)
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
loss
loss(1, 1, 1, 1, 1, x, y)
loss(1, 1, 1, gx, gy, x, y)
gx
x
y
loss(1, 1, 1, gx, gy, x, y)
# obj <- loss.function(y, eta, invlink) + .lasso.penalty(lambda, beta)
#
# if (!is.null(gx)) {
#   obj <- obj + psigx * .edgenet.x.penalty(gx, beta)
# }
# if (!is.null(gy)) {
#   obj <- obj + psigy * .edgenet.y.penalty(gy, beta)
# }
#
# obj
eta
source('~/PROJECTS/netReg/R/loss.R')
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
loss(1, 1, 1, gx, gy, x, y)
loss
alpha
beta
beta <- zero_matrix(p, q) + 1
loss(1, 1, 1, gx, gy, x, y)
tf$matmul(x, beta)
tf$matmul(x, beta) + alpha
loss(alpha, beta, 1, gx, gy, x, y)
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
loss(alpha, beta, 1, gx, gy, x, y)
optimizer <- adam(learning.rate)
learning.rate = 0.01
optimizer <- adam(learning.rate)
train <- optimizer$minimize(objective)
target.old <- Inf
loss
current_loss <- loss(alpha, beta, lambda, gx, gy, x, y)
current_loss
tbeta
beta
d <- t$gradient(current_loss, list(alpha, beta))
d
alpha
beta
current_loss
t
with (tf$GradientTape() %as% t, {
current_loss <- loss(alpha, beta, lambda, gx, gy, x, y)
})
t
d <- t$gradient(current_loss, list(alpha, beta))
d
current_loss
alpha
beta
tensorflow::tf$Variable(tensorflow::tf$zeros(shape(m, n)), trainable = TRUE)
tensorflow::tf$Variable(tensorflow::tf$zeros(shape(10, 10)), trainable = TRUE)
s <- tensorflow::tf$Variable(tensorflow::tf$zeros(shape(10, 10)), trainable = TRUE)
s$trainable
initializer <- tf$keras.initializers$glorot_normal()
tensorflow::tf$Variable(initializer(shape(m)), trainable=trainable)
m <- 10
tensorflow::tf$Variable(initializer(shape(m)), trainable=trainable)
tensorflow::tf$Variable(initializer(shape(m)), trainable=TRUE)
tensorflow::tf$Variable(initializer(shape(m), , tensorflow::tf$float64), trainable=TRUE)
tensorflow::tf$Variable(initializer(shape(m) , tensorflow::tf$float64), trainable=TRUE)
family$linkinv()
family$linkinv
s
family
family()
get.family("gaussian")
s <- get.family("gaussian")
s
s$family
s$link
s$linkinv()
s$linkinv
s <- get.family("binomial")
source('~/PROJECTS/netReg/R/link.R')
s <- get.family("binomial")
s
s$linkinv()
s$linkinv
model <- function(p, q, family) {
keras_model_custom(function(self) {
self$alpha <- init_vector(q)
self$beta <- init_matrix(p, q)
self$family <- family
self$linkinv <- family$linkinv
function(x, mask = NULL, training = FALSE) {
eta <- linear.predictor(self$alpha, self$beta, x)
self$linkinv(eta)
}
})
}
model(10, 1, s)
install.packages("keras")
install.packages("keras")
install.packages()
install.packages("devtools")
install.packages("usethis")
install.packages("usethis")
usethis::use_package("keras")
library(keras)
model <- function(p, q, family) {
keras::keras_model_custom(function(self) {
self$alpha <- init_vector(q)
self$beta <- init_matrix(p, q)
self$family <- family
self$linkinv <- family$linkinv
function(x, mask = NULL, training = FALSE) {
eta <- linear.predictor(self$alpha, self$beta, x)
self$linkinv(eta)
}
})
}
model(10, 1, get.family("binomial"))
source('~/PROJECTS/netReg/R/utils_tf.R')
ss <- model(10, 1, get.family("binomial"))
ss
x
s(x)
ss(x)
mod <- model(p, q, family)
objective <- loss(alpha, beta, lambda, psigx, psigy, x, y)
source('~/PROJECTS/netReg/R/loss.R')
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
loss
loss(mod, 1, 1, 1, x, y)
gx
p
x <- matrix(rnorm(n * p), n)
b <- rnorm(p)
y <- x %*% 5 + rnorm(n)
y
x
y
y <- x %*% 5 + rnorm(n)
y <- x %*% b + rnorm(n)
y
p <- ncol(x)
q <- ncol(y)
x <- cast_float(x)
y <- cast_float(y)
mod <- model(p, q, family)
mod
# estimate coefficients
loss <- edgenet.loss(gx, gy, family)
loss
lambda
psigx
psigy
psigx <- 0
optimizer <- adam(learning.rate)
target.old <- Inf
loss
seq_len(10)
mod
mod$trainable_variables
for (ep in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, vae$trainable_variables
)))
if (step %% 25 == 0) {
target <- sess$run(objective)
if (sum(abs(lo - lo.old)) < thresh) {
break
}
lo.old <- lo
}
}
maxit <- 100
for (ep in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, vae$trainable_variables
)))
if (step %% 25 == 0) {
target <- sess$run(objective)
if (sum(abs(lo - lo.old)) < thresh) {
break
}
lo.old <- lo
}
}
for (ep in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
target <- sess$run(objective)
if (sum(abs(lo - lo.old)) < thresh) {
break
}
lo.old <- lo
}
}
for (step in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
target <- sess$run(objective)
if (sum(abs(lo - lo.old)) < thresh) {
break
}
lo.old <- lo
}
}
for (step in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
if (sum(abs(lo - lo.old)) < thresh) {
break
}
lo.old <- lo
}
}
lo.old <- Inf
for (step in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
if (sum(abs(lo - lo.old)) < thresh) {
break
}
lo.old <- lo
}
}
lo$numpy
lo$numpy()
lo.old <- Inf
for (step in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
if (sum(abs(lo$numpy() - lo.old$numpy())) < thresh) {
break
}
lo.old <- lo
}
}
lo$numpy()mod
lo.old
lo.old <- Inf
lo.old <- Inf
for (step in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
if (sum(abs(lo$numpy() - lo.old)) < thresh) {
break
}
lo.old <- lo$numpy()
}
}
thresh <- 0.0001
lo.old <- Inf
for (step in seq_len(maxit)) {
with(tf$GradientTape() %as% t, {
lo <- loss(mod, lambda, psigx, psigy, x, y)
})
gradients <- t$gradient(lo, mod$trainable_variables)
optimizer$apply_gradients(purrr::transpose(list(
gradients, mod$trainable_variables
)))
if (step %% 25 == 0) {
if (sum(abs(lo$numpy() - lo.old)) < thresh) {
break
}
lo.old <- lo$numpy()
}
}
list(beta = mod$beta, alpha = mod$alpha)
b
library(tensorflow)
library(tfprobability)
n <- 100
p <- 10
psigx <- lambda <- 1
learning.rate = 0.01
psigy <- 0
family <- get.family("gaussian")
g.x <- matrix(runif(p * p), p)
gx <- t(g.x) + g.x
gy <- NULL
x <- matrix(rnorm(n * p), n)
b <- rnorm(p)
y <- x %*% b + rnorm(n)
y
grps <- NULL
grps <- rep(NA_integer_, ncol(X))
X <- x
Y <- y
grps <- rep(NA_integer_, ncol(X))
p <- ncol(x)
q <- ncol(y)
x <- cast_float(x)
y <- cast_float(y)
x
source('~/PROJECTS/netReg/R/utils_tf.R')
cast_float(x)
x <- cast_float(x)
x$shape[1]
x$shape[2]
x$shape[2] == 10
x$get_shape
x$shape
x$shape[[1]]
x$shape[[2]]
colnames(x)
tfprobability::tfd_poisson(rate = invlink(eta[, j]))
library(tfprobability)
tfprobability::tfd_poisson(rate = x)
tfprobability::tfd_poisson(rate = x**2)
library(tensorflow)
library(tfprobability
)
tensorflow::tf_version()
tensorflow::tf_version
install.packages("tensorflow")
install.packages("tensorflow")
install.packages("tfprobability")
tensorflow::install_tensorflow("conda", envname = "r-reticulate")
